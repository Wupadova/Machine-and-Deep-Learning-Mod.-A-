{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wupadova/Machine-and-Deep-Learning-Mod.-A-/blob/main/ML_project_update.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66768ff7",
      "metadata": {
        "id": "66768ff7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from urllib.request import urlopen\n",
        "\n",
        "def load_fashion_mnist():\n",
        "\n",
        "    url_base = \"https://www.math.unipd.it/~dasan/\"\n",
        "    Y_train = np.frombuffer(urlopen(url_base + \"train-labels-idx1-ubyte\").read(), dtype=np.uint8, offset=8)\n",
        "    X_train = np.frombuffer(urlopen(url_base + \"train-images-idx3-ubyte\").read(), dtype=np.uint8, offset=16).reshape(len(Y_train), 784) # besides loadng \n",
        "                                                                                            #the data, I already flatten it into a vector\n",
        "    Y_test = np.frombuffer(urlopen(url_base + \"t10k-labels-idx1-ubyte\").read(), dtype=np.uint8, offset=8)\n",
        "    X_test = np.frombuffer(urlopen(url_base + \"t10k-images-idx3-ubyte\").read(), dtype=np.uint8, offset=16).reshape(len(Y_test), 784)\n",
        "\n",
        "    return X_train, Y_train, X_test, Y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6b9de0d",
      "metadata": {
        "id": "d6b9de0d",
        "outputId": "e5e3051a-1694-4517-c0a1-3fac6dcbc01e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 784) (60000,) (10000, 784) (10000,)\n"
          ]
        }
      ],
      "source": [
        "X_train, Y_train, X_test, Y_test = load_fashion_mnist() # you can use this function (copying the whole box) in your code to easily load the data\n",
        "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0e537b7",
      "metadata": {
        "id": "a0e537b7"
      },
      "outputs": [],
      "source": [
        "# split the train and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_, X_val, y_train_, y_val = train_test_split(X_train, Y_train, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfd10e33",
      "metadata": {
        "id": "cfd10e33",
        "outputId": "47ba2c30-e5d4-4566-9813-2822bac0ce3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(48000, 784) (12000, 784) (10000, 784)\n"
          ]
        }
      ],
      "source": [
        "# check if the data set is split correctly\n",
        "print(X_train_.shape, X_val.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51016810",
      "metadata": {
        "id": "51016810",
        "outputId": "a3433a40-c546-48e9-c9d7-4747e48e4af6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n",
            "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
          ]
        }
      ],
      "source": [
        "# check the output\n",
        "print(len(set(Y_train)))\n",
        "print(set(Y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06342fc7",
      "metadata": {
        "id": "06342fc7",
        "outputId": "8b677b7c-1722-42a2-ab43-01c1dc8d347e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "print(X_train[0:5, 0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bbe48cd",
      "metadata": {
        "id": "5bbe48cd",
        "outputId": "052e60b3-ce13-4e9a-ff67-fe58efbdcc2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[9 0 0 3 0]\n"
          ]
        }
      ],
      "source": [
        "print(Y_train[0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b565985",
      "metadata": {
        "id": "7b565985"
      },
      "outputs": [],
      "source": [
        "# try with different models: decision tree, random forest, SVC and in the end NN starting with raw data\n",
        "# import all the possibly necessary libraries \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "import tensorflow #the backend used by Keras (there are different beckend)\n",
        "from tensorflow.keras.models import Sequential #import the type of mpdel: sequential (e.g., MLP)\n",
        "from tensorflow.keras.layers import Input, Dense #simple linear layer\n",
        "from tensorflow.keras.utils import to_categorical # transformation for classification labels\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.python.framework.random_seed import set_random_seed\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import EarlyStopping\n",
        "import joblib\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e91baa27",
      "metadata": {
        "id": "e91baa27"
      },
      "source": [
        "## Try the models with raw data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a22860cf",
      "metadata": {
        "id": "a22860cf"
      },
      "outputs": [],
      "source": [
        "# decision tree \n",
        "\n",
        "dt_grid_param = {\n",
        "    'criterion' : ['entropy'],\n",
        "    'max_depth' : (None, 3, 5, 7)   \n",
        "}\n",
        "\n",
        "dt_clf = DecisionTreeClassifier(random_state=123)\n",
        "dt_v1 = GridSearchCV(dt_clf, dt_grid_param, n_jobs=-1, cv=5)\n",
        "dt_v1.fit(X_train_, y_train_)\n",
        "\n",
        "# random forest \n",
        "\n",
        "rf_grid_param = {\n",
        "    'n_estimators' : (5, 10, 20),\n",
        "    'criterion' : ['entropy'],\n",
        "    'max_depth' : (None, 3, 5, 7)\n",
        "}\n",
        "\n",
        "rf_clf = RandomForestClassifier(random_state=123)\n",
        "rf_v1 = GridSearchCV(rf_clf, rf_grid_param, n_jobs=-1, cv=5)\n",
        "rf_v1.fit(X_train_, y_train_)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4053f306",
      "metadata": {
        "id": "4053f306"
      },
      "outputs": [],
      "source": [
        "# try with knn classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_clf = KNeighborsClassifier()\n",
        "\n",
        "knn_grid_param = {\n",
        "    'n_neighbors' : (5, 10, 20),\n",
        "    'weights' : ('uniform', 'distance')\n",
        "}\n",
        "\n",
        "knn_v1 = GridSearchCV(knn_clf, knn_grid_param, n_jobs=-1, cv=5)\n",
        "knn_v1.fit(X_train_, y_train_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68954a01",
      "metadata": {
        "id": "68954a01"
      },
      "outputs": [],
      "source": [
        "# extra-tree classifier \n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "\n",
        "ext_tree_clf = ExtraTreeClassifier(random_state=123)\n",
        "\n",
        "ext_tree_param = {\n",
        "    'criterion' : ['entropy'],\n",
        "    'splitter' : ['random', 'best'],\n",
        "}\n",
        "\n",
        "ext_tree_v1 = GridSearchCV(ext_tree_clf, ext_tree_param, n_jobs=-1, cv=5)\n",
        "ext_tree_v1.fit(X_train_, y_train_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a57a06a1",
      "metadata": {
        "id": "a57a06a1"
      },
      "outputs": [],
      "source": [
        "# try with OVO and OVR strategies\n",
        "# OVO and OVR for perceptron\n",
        "model = Perceptron(fit_intercept=True, n_jobs=-1, random_state=123)\n",
        "\n",
        "# define OVO strategy and train\n",
        "ovo = OneVsOneClassifier(model)\n",
        "ovo.fit(X_train_, y_train_)\n",
        "\n",
        "# define OVR strategy and train\n",
        "ovr = OneVsRestClassifier(model)\n",
        "ovr.fit(X_train_, y_train_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91b50c11",
      "metadata": {
        "scrolled": true,
        "id": "91b50c11"
      },
      "outputs": [],
      "source": [
        "# NN \n",
        "# define the early stopping\n",
        "es = EarlyStopping(monitor='val_loss', #quantity to be monitored\n",
        "                   mode='min', #we look for decreasing patterns stop \n",
        "                   patience = 3, #number of epochs with no improvement\n",
        "                   verbose=1)\n",
        "\n",
        "# define the NN settings\n",
        "feature_vector_length = len(X_train_[0])  # assign the number of features\n",
        "num_classes = len(set(Y_train))   # assign the number of the classes\n",
        "print(feature_vector_length, num_classes)  # check the result\n",
        "\n",
        "# convert the ground truth from numerical to categorical representation\n",
        "y_train_cat = to_categorical(y_train_, num_classes)\n",
        "y_val_cat = to_categorical(y_val, num_classes)\n",
        "\n",
        "# set the seed\n",
        "np.random.seed(123)\n",
        "set_random_seed(2)\n",
        "\n",
        "# define the NN model\n",
        "model = Sequential()   # define how the 'model' looks like\n",
        "model.add(Dense(input_dim=feature_vector_length, units=feature_vector_length, activation='relu'))  # input layer\n",
        "model.add(Dense(num_classes, activation='softmax'))  # output layer\n",
        "\n",
        "# configure the model and start training \n",
        "model.compile(loss='categorical_crossentropy',   # loss metric\n",
        "             optimizer='sgd',   # optimizer\n",
        "             metrics=['accuracy'])  # displayed metric\n",
        "\n",
        "history = model.fit(X_train_, y_train_cat, epochs=500, batch_size=32, verbose=1, validation_split=0.1, callbacks=[es])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e0640f7",
      "metadata": {
        "id": "1e0640f7"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "# plt.ylim(0.8, 1)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adc41240",
      "metadata": {
        "id": "adc41240"
      },
      "outputs": [],
      "source": [
        "# check the result of the above two models (original dataset is too large and too complicated\n",
        "# for the SVM, thus we will perform a pre-processing later for the SVM)\n",
        "# decision\n",
        "y_train_pred = dt_v1.predict(X_train_)\n",
        "y_val_pred = dt_v1.predict(X_val)\n",
        "print(f\"Decision Tree.\\tTrain:{f1_score(y_train_, y_train_pred,average = 'macro'):.4f}\\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}\")\n",
        "\n",
        "# random forest\n",
        "y_train_pred = rf_v1.predict(X_train_)\n",
        "y_val_pred = rf_v1.predict(X_val)\n",
        "print(f\"Random Forest.\\tTrain:{f1_score(y_train_, y_train_pred,average = 'macro'):.4f}\\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}\")\n",
        "\n",
        "# knn evaluation\n",
        "y_train_pred = knn_v1.predict(X_train_)\n",
        "y_val_pred = knn_v1.predict(X_val)\n",
        "print(f\"KNN.\\tTrain:{f1_score(y_train_, y_train_pred,average = 'macro'):.4f}\\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}\")\n",
        "\n",
        "# extra tree evaluation\n",
        "y_train_pred = ext_tree_v1.predict(X_train_)\n",
        "y_val_pred = ext_tree_v1.predict(X_val)\n",
        "print(f\"Extra Tree.\\tTrain:{f1_score(y_train_, y_train_pred,average = 'macro'):.4f}\\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}\")\n",
        "\n",
        "# ovo perceptron evaluation\n",
        "y_train_pred = ovo.predict(X_train_)\n",
        "y_val_pred = ovo.predict(X_val)\n",
        "print(f\"OVO_perceptron.\\tTrain:{f1_score(y_train_, y_train_pred,average = 'macro'):.4f}\\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}\")\n",
        "\n",
        "# ovr perceptron evaluation\n",
        "y_train_pred = ovr.predict(X_train_)\n",
        "y_val_pred = ovr.predict(X_val)\n",
        "print(f\"OVR_perceptron.\\tTrain:{f1_score(y_train_, y_train_pred,average = 'macro'):.4f}\\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "636d804e",
      "metadata": {
        "id": "636d804e"
      },
      "outputs": [],
      "source": [
        "# show the best hpyer parameters\n",
        "print('Decison Tree:', dt_v1.best_params_)\n",
        "print('Random Forest:', rf_v1.best_params_)\n",
        "print('Extra Tree:', ext_tree_v1.best_params_)\n",
        "print('KNN:', knn_v1.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb6c9255",
      "metadata": {
        "id": "eb6c9255"
      },
      "source": [
        "## Some models are overfitted, we check the data to see if they are balanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21ecc632",
      "metadata": {
        "id": "21ecc632"
      },
      "outputs": [],
      "source": [
        "# first we check the Y dataset\n",
        "for a in set(y_train_):\n",
        "    result = 0\n",
        "    for b in range(len(y_train_)):\n",
        "        if y_train_[b] == a:\n",
        "            result += 1\n",
        "    print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "833874fc",
      "metadata": {
        "id": "833874fc"
      },
      "source": [
        "## The training set is well balanced, thus we can try to perform the pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a41ec59",
      "metadata": {
        "id": "4a41ec59"
      },
      "outputs": [],
      "source": [
        "# try with MinMaxScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# define the scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "scaler.fit(X_train_)   # fit the scaler with the training set\n",
        "\n",
        "# transform the dataset\n",
        "X_train_scaled = scaler.transform(X_train_)  # scaled training set\n",
        "X_val_scaled = scaler.transform(X_val)  # scaled validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa98fd42",
      "metadata": {
        "id": "fa98fd42"
      },
      "outputs": [],
      "source": [
        "# train the models with the scaled data\n",
        "# decision tree\n",
        "dt_v1.fit(X_train_scaled, y_train_)\n",
        "\n",
        "# random forest \n",
        "rf_v1.fit(X_train_scaled, y_train_)\n",
        "\n",
        "# extra tree\n",
        "ext_tree_v1.fit(X_train_scaled, y_train_)\n",
        "\n",
        "# knn\n",
        "knn_v1.fit(X_train_scaled, y_train_)\n",
        "\n",
        "# OVO strategy for perceptron\n",
        "ovo.fit(X_train_scaled, y_train_)\n",
        "\n",
        "# OVR strategy for perceptron\n",
        "ovr.fit(X_train_scaled, y_train_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82a53880",
      "metadata": {
        "id": "82a53880"
      },
      "outputs": [],
      "source": [
        "# evaluate the result\n",
        "# decision\n",
        "y_train_pred = dt_v1.predict(X_train_scaled)\n",
        "y_val_pred = dt_v1.predict(X_val_scaled)\n",
        "print(f\"Decision Tree.\\tTrain:{f1_score(y_train_, y_train_pred,average = 'macro'):.4f}\\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}\")\n",
        "\n",
        "# random forest\n",
        "y_train_pred = rf_v1.predict(X_train_scaled)\n",
        "y_val_pred = rf_v1.predict(X_val_scaled)\n",
        "print(f\"Random Forest.\\tTrain:{f1_score(y_train_, y_train_pred,average = 'macro'):.4f}\\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}\")\n",
        "\n",
        "# knn evaluation\n",
        "y_train_pred = knn_v1.predict(X_train_scaled)\n",
        "y_val_pred = knn_v1.predict(X_val_scaled)\n",
        "print(f\"KNN.\\tTrain:{f1_score(y_train_, y_train_pred,average = 'macro'):.4f}\\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}\")\n",
        "\n",
        "# extra tree evaluation\n",
        "y_train_pred = ext_tree_v1.predict(X_train_scaled)\n",
        "y_val_pred = ext_tree_v1.predict(X_val_scaled)\n",
        "print(f\"Extra Tree.\\tTrain:{f1_score(y_train_, y_train_pred,average = 'macro'):.4f}\\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}\")\n",
        "\n",
        "# ovo perceptron evaluation\n",
        "y_train_pred = ovo.predict(X_train_scaled)\n",
        "y_val_pred = ovo.predict(X_val_scaled)\n",
        "print(f\"OVO_perceptron.\\tTrain:{f1_score(y_train_, y_train_pred,average = 'macro'):.4f}\\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}\")\n",
        "\n",
        "# ovr perceptron evaluation\n",
        "y_train_pred = ovr.predict(X_train_scaled)\n",
        "y_val_pred = ovr.predict(X_val_scaled)\n",
        "print(f\"OVR_perceptron.\\tTrain:{f1_score(y_train_, y_train_pred,average = 'macro'):.4f}\\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c5d7d0d",
      "metadata": {
        "id": "7c5d7d0d"
      },
      "outputs": [],
      "source": [
        "# show the best hpyer parameters\n",
        "print('Decison Tree:', dt_v1.best_params_)\n",
        "print('Random Forest:', rf_v1.best_params_)\n",
        "print('Extra Tree:', ext_tree_v1.best_params_)\n",
        "print('KNN:', knn_v1.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bd4229d",
      "metadata": {
        "id": "2bd4229d"
      },
      "outputs": [],
      "source": [
        "# NN\n",
        "history = model.fit(X_train_scaled, y_train_cat, epochs=500, batch_size=32, verbose=1, validation_split=0.1, callbacks=[es])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2bc451c",
      "metadata": {
        "id": "f2bc451c"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "# plt.ylim(0.8, 1)\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d21acbf4",
      "metadata": {
        "id": "d21acbf4"
      },
      "source": [
        "## Now, we can see that the NN model improved a lot, however, other models are less 'sensitive' to the scaler.  Now, we can try the SVC and logistic regression but the PCA is needed beforehand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "931ec682",
      "metadata": {
        "id": "931ec682"
      },
      "outputs": [],
      "source": [
        "# define the PCA and set the number of components\n",
        "pca = PCA(n_components=0.95, random_state=123)\n",
        "\n",
        "# fit and transform the data\n",
        "pca.fit(X_train_scaled)   # fit the pca with scaled training set\n",
        "X_train_reduced = pca.transform(X_train_scaled)   # transform the training set\n",
        "X_val_reduced = pca.transform(X_val_scaled)  # transform the validation set\n",
        "\n",
        "# check the reduced shape of our data\n",
        "print(X_train_reduced.shape)\n",
        "print(X_val_reduced.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb57156a",
      "metadata": {
        "id": "fb57156a"
      },
      "source": [
        "## Now, we can try all the models again with the reduced data and see the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d39b0a9",
      "metadata": {
        "id": "2d39b0a9"
      },
      "outputs": [],
      "source": [
        "# train the models with the reduced data\n",
        "# decision tree\n",
        "dt_v1.fit(X_train_reduced, y_train_)\n",
        "\n",
        "# random forest \n",
        "rf_v1.fit(X_train_reduced, y_train_)\n",
        "\n",
        "# extra tree\n",
        "ext_tree_v1.fit(X_train_reduced, y_train_)\n",
        "\n",
        "# knn\n",
        "knn_v1.fit(X_train_reduced, y_train_)\n",
        "\n",
        "# OVO strategy for perceptron\n",
        "ovo.fit(X_train_reduced, y_train_)\n",
        "\n",
        "# OVR strategy for perceptron\n",
        "ovr.fit(X_train_reduced, y_train_)\n",
        "\n",
        "# OVO and OVR strategies for logistic regression\n",
        "model_lr = LogisticRegression(fit_intercept=True, n_jobs=-1, random_state=123)\n",
        "\n",
        "# define OVO strategy and train\n",
        "ovo_lr = OneVsOneClassifier(model_lr)\n",
        "ovo_lr.fit(X_train_reduced, y_train_)\n",
        "\n",
        "# define OVR strategy and train\n",
        "ovr_lr = OneVsRestClassifier(model_lr)\n",
        "ovr_lr.fit(X_train_reduced, y_train_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b0a3e58",
      "metadata": {
        "id": "4b0a3e58"
      },
      "outputs": [],
      "source": [
        "# evaluate the result\n",
        "# decision\n",
        "y_train_pred = dt_v1.predict(X_train_reduced)\n",
        "y_val_pred = dt_v1.predict(X_val_reduced)\n",
        "print(f\"Decision Tree.\\tTrain:{f1_score(y_train_, y_train_pred,average = 'macro'):.4f}\\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}\")\n",
        "\n",
        "# random forest\n",
        "y_train_pred = rf_v1.predict(X_train_reduced)\n",
        "y_val_pred = rf_v1.predict(X_val_reduced)\n",
        "print(f\"Random Forest.\\tTrain:{f1_score(y_train_, y_train_pred,average = 'macro'):.4f}\\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}\")\n",
        "\n",
        "# knn evaluation\n",
        "y_train_pred = knn_v1.predict(X_train_reduced)\n",
        "y_val_pred = knn_v1.predict(X_val_reduced)\n",
        "print(f\"KNN.\\tTrain:{f1_score(y_train_, y_train_pred,average = 'macro'):.4f}\\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}\")\n",
        "\n",
        "# extra tree evaluation\n",
        "y_train_pred = ext_tree_v1.predict(X_train_reduced)\n",
        "y_val_pred = ext_tree_v1.predict(X_val_reduced)\n",
        "print(f\"Extra Tree.\\tTrain:{f1_score(y_train_, y_train_pred,average = 'macro'):.4f}\\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}\")\n",
        "\n",
        "# ovo perceptron evaluation\n",
        "y_train_pred = ovo.predict(X_train_reduced)\n",
        "y_val_pred = ovo.predict(X_val_reduced)\n",
        "print(f\"OVO_perceptron.\\tTrain:{f1_score(y_train_, y_train_pred,average = 'macro'):.4f}\\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}\")\n",
        "\n",
        "# ovr perceptron evaluation\n",
        "y_train_pred = ovr.predict(X_train_reduced)\n",
        "y_val_pred = ovr.predict(X_val_reduced)\n",
        "print(f\"OVR_perceptron.\\tTrain:{f1_score(y_train_, y_train_pred,average = 'macro'):.4f}\\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}\")\n",
        "\n",
        "# ovo logistic regression evaluation\n",
        "y_train_pred = ovo_lr.predict(X_train_reduced)\n",
        "y_val_pred = ovo_lr.predict(X_val_reduced)\n",
        "print(f\"OVO_logistic regression.\\tTrain:{f1_score(y_train_, y_train_pred,average = 'macro'):.4f}\\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}\")\n",
        "\n",
        "# ovr logistic regression evaluation\n",
        "y_train_pred = ovr_lr.predict(X_train_reduced)\n",
        "y_val_pred = ovr_lr.predict(X_val_reduced)\n",
        "print(f\"OVR_logistric regression.\\tTrain:{f1_score(y_train_, y_train_pred,average = 'macro'):.4f}\\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7717c674",
      "metadata": {
        "id": "7717c674"
      },
      "source": [
        "## The last two rows are not perceptron but logistic regression, it takes some time to re-run the previous cell, thus, for the sake of time saving, I will keep it as it is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afb13235",
      "metadata": {
        "id": "afb13235"
      },
      "outputs": [],
      "source": [
        "# show the best hpyer parameters\n",
        "print('Decison Tree:', dt_v1.best_params_)\n",
        "print('Random Forest:', rf_v1.best_params_)\n",
        "print('Extra Tree:', ext_tree_v1.best_params_)\n",
        "print('KNN:', knn_v1.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cf54438",
      "metadata": {
        "id": "3cf54438"
      },
      "outputs": [],
      "source": [
        "# NN\n",
        "# define the NN settings\n",
        "feature_vector_length = len(X_train_reduced[0])  # assign the number of features\n",
        "num_classes = len(set(Y_train))   # assign the number of the classes\n",
        "print(feature_vector_length, num_classes)  # check the result\n",
        "\n",
        "# convert the ground truth from numerical to categorical representation\n",
        "y_train_cat = to_categorical(y_train_, num_classes)\n",
        "y_val_cat = to_categorical(y_val, num_classes)\n",
        "\n",
        "# set the seed\n",
        "np.random.seed(123)\n",
        "set_random_seed(2)\n",
        "\n",
        "# define the NN model\n",
        "model = Sequential()   # define how the 'model' looks like\n",
        "model.add(Dense(input_dim=feature_vector_length, units=feature_vector_length, activation='relu'))  # input layer\n",
        "model.add(Dense(num_classes, activation='softmax'))  # output layer\n",
        "\n",
        "# configure the model and start training \n",
        "model.compile(loss='categorical_crossentropy',   # loss metric\n",
        "             optimizer='sgd',   # optimizer\n",
        "             metrics=['accuracy'])  # displayed metric\n",
        "\n",
        "history = model.fit(X_train_reduced, y_train_cat, epochs=500, batch_size=32, verbose=1, validation_split=0.1, callbacks=[es])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff9a9c9c",
      "metadata": {
        "id": "ff9a9c9c"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "# plt.ylim(0.8, 1)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64ed6ada",
      "metadata": {
        "id": "64ed6ada"
      },
      "outputs": [],
      "source": [
        "# train the SVM \n",
        "\n",
        "svc_grid_param = {\n",
        "    'C' : (0.1, 1, 10, 15, 20),\n",
        "    'kernel' : ('rbf', 'linear')\n",
        "}\n",
        "\n",
        "svc_clf = SVC(random_state=123)\n",
        "svc_v1 = GridSearchCV(svc_clf, svc_grid_param, n_jobs=-1, cv=5)\n",
        "svc_v1.fit(X_train_reduced, y_train_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05191881",
      "metadata": {
        "id": "05191881"
      },
      "outputs": [],
      "source": [
        "# svc\n",
        "y_train_pred = svc_v1.predict(X_train_reduced)\n",
        "y_val_pred = svc_v1.predict(X_val_reduced)\n",
        "print(f\"SVC.\\tTrain:{f1_score(y_train_, y_train_pred,average = 'macro'):.4f}\\tVal:{f1_score(y_val, y_val_pred,average = 'macro'):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1d70285",
      "metadata": {
        "id": "a1d70285"
      },
      "outputs": [],
      "source": [
        "print('SVC:', svc_v1.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dda459c4",
      "metadata": {
        "id": "dda459c4"
      },
      "source": [
        "## The first four models remain similar performances, but the extra tree has a worse performance. As for the NN and SVC, We got a decent results, but both models are overfitted now, recall that without PCA, the difference between training and validation of the NN model is negligible. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}